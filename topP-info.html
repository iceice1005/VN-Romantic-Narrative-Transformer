
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Top-P (Nucleus Sampling)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body { font-family: 'Times New Roman', Times, serif; background-color: #fffaf0; padding: 2rem; color: #333; }
        h1 { color: #1565c0; } /* Darker blue */
        h2 { color: #6a1b9a; margin-top: 1.5rem; } /* Dark Purple */
        code { background-color: #e3f2fd; color: #0d47a1; padding: 0.2rem 0.4rem; border-radius: 4px; font-family: monospace; }
        p { line-height: 1.6; }
        .container { max-width: 800px; margin: auto; background-color: #ffffff; padding: 2rem; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
    </style>
</head>
<body>
    <div class="container">
        <h1>Understanding Top-P (Nucleus Sampling)</h1>
        <p><strong>Top-P</strong>, also known as Nucleus Sampling, is a parameter that controls the diversity of the text generated by the model. It works by selecting from the smallest set of most probable next words (the "nucleus") whose cumulative probability exceeds the value <code>P</code>.</p>

        <h2>How it Works</h2>
        <p>Instead of considering all possible next words or a fixed number (like Top-K), Top-P dynamically adjusts the number of words considered based on their probabilities:</p>
        <ul>
            <li>The model calculates probabilities for all possible next words.</li>
            <li>It then sorts these words by probability in descending order.</li>
            <li>It accumulates these probabilities until the sum is greater than or equal to <code>P</code>. The set of words included up to this point forms the "nucleus."</li>
            <li>The model then samples the next word only from this nucleus (often in conjunction with Temperature).</li>
        </ul>
        <p>For example, if <code>topP = 0.9</code>: The model considers the most likely words that together make up 90% of the probability mass for the next word. This filters out the long tail of very unlikely words.</p>

        <h2>Effect on Romantic Narrative Transformation</h2>
        <p>Top-P helps in controlling the "focus" and "coherence" of the generated narrative:</p>
        <ul>
            <li><strong>Lower Top-P (e.g., <code>0.8</code>):</strong>
                <p>The model considers a smaller, more probable set of next words. This leads to more focused and predictable text, reducing the chances of very unusual or off-topic word choices. The narrative might feel more conservative or 'safer'.</p>
                <p><em>Example Idea:</em> If the context is a tender farewell, a lower Top-P might ensure the vocabulary stays strictly within common expressions of sadness and longing, avoiding unexpected turns of phrase.</p>
            </li>
            <li><strong>Higher Top-P (e.g., <code>0.95</code> - <code>1.0</code>):</strong>
                <p>The model considers a wider range of words, including some less probable ones. This can lead to more diverse, creative, and sometimes surprising outputs. If set to <code>1.0</code>, it considers all words (though Temperature would still influence sampling).</p>
                <p><em>Example Idea:</em> For the same farewell, a higher Top-P might allow for more unique metaphors or slightly unconventional descriptions of sorrow, potentially making the narrative more distinctive but also slightly increasing the risk of less relevant phrasing if not well-managed by the prompt and Temperature.</p>
            </li>
        </ul>
        <p>Compared to Temperature, Top-P is more about dynamically limiting the *pool* of candidate words, while Temperature is about reshaping the probabilities *within* that pool (or the entire vocabulary if Top-P is high).</p>

        <h2>Recommendation for This App</h2>
        <p>A common default value for Top-P is around <strong><code>0.9</code> to <code>0.95</code></strong>. This usually provides a good balance between coherence and creativity.</p>
        <ul>
            <li>If the output feels too repetitive or constrained, you could try slightly increasing Top-P (e.g., to <code>0.98</code>).</li>
            <li>If the output contains too many strange or irrelevant word choices, try decreasing Top-P (e.g., to <code>0.85</code>).</li>
        </ul>
        <p>It's often best to adjust Top-P in conjunction with Temperature to fine-tune the output quality.</p>
    </div>
</body>
</html>
